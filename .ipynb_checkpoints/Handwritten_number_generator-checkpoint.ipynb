{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digit Generator\n",
    "In this notebook, we will build a ganerative adversarial network to generate handwritten digit. Thanks to Udacity providing study material!\n",
    "\n",
    "## Brief Intro on GAN network\n",
    "The idea behind GANs is that there are two networks - a generator G and a discriminator D - competing against each other. The generator create fake data to pass to the discriminator. The discriminator also have sees real data and judge whether input data is fake or real. The purpose of generator is to fool the discriminator, which means to create data that looks as close as possible to real data. The purpose of discriminator is to figure out which data is real and which is fake. At the end of training, generator is able to create data very close to real ones. \n",
    "\n",
    "The visualization is attached below:\n",
    "\n",
    "![GAN_Diagram.png](https://c1.staticflickr.com/1/973/27850668338_4671349d77_b.jpg)\n",
    "\n",
    "Now, let us build this GAN network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Firstly, import packages and data!\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inputs\n",
    "After importing data, it is the time to create model inputs. For GAN network, we need two inputs, one for the discriminator - inputs_real and one for the generator - inputs_fake. In the model inputs function, we set appropriate sizes and placeholder for inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, fake_dim):\n",
    "    inputs_real = tf.placeholder(tf.float32, (None, real_dim), name='input_real')\n",
    "    inputs_fake = tf.placeholder(tf.float32, (None, fake_dim), name='input_fake')\n",
    "    return inputs_real, inputs_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN structure\n",
    "In this part, we start to build the network, looks like below\n",
    "![GAN.png](https://c1.staticflickr.com/1/977/39912695390_934e054ff8_b.jpg)\n",
    "In GAN network, we use Leaky ReLu for our hidden layer to prevent dying nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create discriminator\n",
    "def discriminator(x, n_units=128, reuse=False, alpha=0.01):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Hidden layer\n",
    "        h1 = tf.layers.dense(x, n_units, activation=None)\n",
    "        # Add leaky relu as activation function\\\n",
    "        h1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        logits = tf.layers.dense(h1, 1, activation=None)\n",
    "        out = tf.sigmoid(logits)\n",
    "        \n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create generator\n",
    "def generator(z, out_dim, n_units=128, reuse=False, alpha=0.01):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # Hidden layer\n",
    "        h1 = tf.layers.dense(z, n_units, activation=None)\n",
    "        h1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "        logits = tf.layers.dense(h1, out_dim, activation=None)\n",
    "        out = tf.tanh(logits)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "After creating discriminator and generator, we set corresponding hyperparameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 28 * 28\n",
    "z_size = 100\n",
    "# Size of hidden layers for generator and discriminator\n",
    "g_hidden_size = 128\n",
    "d_hidden_size = 128\n",
    "# parameter for leaky ReLU\n",
    "alpha = 0.01\n",
    "# Smoothing\n",
    "smooth = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# Create input placeholder\n",
    "input_real, input_z = model_inputs(input_size, z_size)\n",
    "\n",
    "# Build the model\n",
    "g_model = generator(input_z, input_size, n_units=g_hidden_size, alpha=alpha)\n",
    "\n",
    "# Two discriminators, one for real data, one for fake data\n",
    "d_model_real, d_logits_real = discriminator(input_real, n_units=d_hidden_size, alpha=alpha)\n",
    "d_model_fake, d_logits_fake = discriminator(g_model, reuse=True, n_units=d_hidden_size, alpha=alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator and Generator Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_loss_real = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n",
    "                                                        labels=tf.ones_like(\n",
    "                                                        d_logits_real) * (1 - smooth)))\n",
    "d_loss_fake = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                        labels=tf.zeros_like(d_logits_real)))\n",
    "\n",
    "d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "g_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                   labels=tf.ones_like(d_logits_fake)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "\n",
    "t_vars = tf.trainable_variables()\n",
    "g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100... Discriminator Loss: 0.3597... Generator Loss: 4.5672...\n",
      "Epoch 2/100... Discriminator Loss: 0.4643... Generator Loss: 2.2357...\n",
      "Epoch 3/100... Discriminator Loss: 0.4614... Generator Loss: 3.2755...\n",
      "Epoch 4/100... Discriminator Loss: 0.9195... Generator Loss: 2.8062...\n",
      "Epoch 5/100... Discriminator Loss: 0.8920... Generator Loss: 3.1900...\n",
      "Epoch 6/100... Discriminator Loss: 1.7980... Generator Loss: 3.1499...\n",
      "Epoch 7/100... Discriminator Loss: 0.8014... Generator Loss: 2.4445...\n",
      "Epoch 8/100... Discriminator Loss: 0.9009... Generator Loss: 3.6422...\n",
      "Epoch 9/100... Discriminator Loss: 1.6549... Generator Loss: 1.4561...\n",
      "Epoch 10/100... Discriminator Loss: 0.7660... Generator Loss: 2.5701...\n",
      "Epoch 11/100... Discriminator Loss: 1.0124... Generator Loss: 1.7516...\n",
      "Epoch 12/100... Discriminator Loss: 1.1846... Generator Loss: 2.7244...\n",
      "Epoch 13/100... Discriminator Loss: 1.4394... Generator Loss: 1.9745...\n",
      "Epoch 14/100... Discriminator Loss: 0.9290... Generator Loss: 1.8892...\n",
      "Epoch 15/100... Discriminator Loss: 0.8477... Generator Loss: 3.1487...\n",
      "Epoch 16/100... Discriminator Loss: 1.1999... Generator Loss: 1.6133...\n",
      "Epoch 17/100... Discriminator Loss: 0.9929... Generator Loss: 2.5559...\n",
      "Epoch 18/100... Discriminator Loss: 1.4359... Generator Loss: 1.9776...\n",
      "Epoch 19/100... Discriminator Loss: 0.9717... Generator Loss: 2.6356...\n",
      "Epoch 20/100... Discriminator Loss: 0.9955... Generator Loss: 2.0029...\n",
      "Epoch 21/100... Discriminator Loss: 1.2828... Generator Loss: 1.6559...\n",
      "Epoch 22/100... Discriminator Loss: 1.2730... Generator Loss: 1.2492...\n",
      "Epoch 23/100... Discriminator Loss: 1.2711... Generator Loss: 1.3384...\n",
      "Epoch 24/100... Discriminator Loss: 0.9648... Generator Loss: 1.6233...\n",
      "Epoch 25/100... Discriminator Loss: 0.9893... Generator Loss: 1.6823...\n",
      "Epoch 26/100... Discriminator Loss: 1.0837... Generator Loss: 2.1539...\n",
      "Epoch 27/100... Discriminator Loss: 1.0399... Generator Loss: 1.4450...\n",
      "Epoch 28/100... Discriminator Loss: 0.8351... Generator Loss: 2.4024...\n",
      "Epoch 29/100... Discriminator Loss: 0.9397... Generator Loss: 2.1238...\n",
      "Epoch 30/100... Discriminator Loss: 1.1684... Generator Loss: 1.9939...\n",
      "Epoch 31/100... Discriminator Loss: 0.8299... Generator Loss: 2.0050...\n",
      "Epoch 32/100... Discriminator Loss: 0.9150... Generator Loss: 2.5894...\n",
      "Epoch 33/100... Discriminator Loss: 0.7798... Generator Loss: 2.6446...\n",
      "Epoch 34/100... Discriminator Loss: 0.8461... Generator Loss: 2.0757...\n",
      "Epoch 35/100... Discriminator Loss: 1.0353... Generator Loss: 1.9935...\n",
      "Epoch 36/100... Discriminator Loss: 0.9521... Generator Loss: 1.8448...\n",
      "Epoch 37/100... Discriminator Loss: 0.8155... Generator Loss: 2.3032...\n",
      "Epoch 38/100... Discriminator Loss: 1.0059... Generator Loss: 1.9014...\n",
      "Epoch 39/100... Discriminator Loss: 1.0603... Generator Loss: 1.9270...\n",
      "Epoch 40/100... Discriminator Loss: 0.8487... Generator Loss: 1.9401...\n",
      "Epoch 41/100... Discriminator Loss: 0.7542... Generator Loss: 2.5836...\n",
      "Epoch 42/100... Discriminator Loss: 1.5284... Generator Loss: 1.7507...\n",
      "Epoch 43/100... Discriminator Loss: 0.7444... Generator Loss: 2.3938...\n",
      "Epoch 44/100... Discriminator Loss: 0.9212... Generator Loss: 2.1404...\n",
      "Epoch 45/100... Discriminator Loss: 0.9240... Generator Loss: 2.3345...\n",
      "Epoch 46/100... Discriminator Loss: 0.9926... Generator Loss: 1.6303...\n",
      "Epoch 47/100... Discriminator Loss: 1.0292... Generator Loss: 1.9553...\n",
      "Epoch 48/100... Discriminator Loss: 1.1581... Generator Loss: 1.7284...\n",
      "Epoch 49/100... Discriminator Loss: 0.8896... Generator Loss: 1.9906...\n",
      "Epoch 50/100... Discriminator Loss: 1.0305... Generator Loss: 1.9365...\n",
      "Epoch 51/100... Discriminator Loss: 0.8935... Generator Loss: 1.9685...\n",
      "Epoch 52/100... Discriminator Loss: 0.9021... Generator Loss: 2.0808...\n",
      "Epoch 53/100... Discriminator Loss: 0.9029... Generator Loss: 2.3778...\n",
      "Epoch 54/100... Discriminator Loss: 0.8363... Generator Loss: 2.4716...\n",
      "Epoch 55/100... Discriminator Loss: 1.0002... Generator Loss: 1.7945...\n",
      "Epoch 56/100... Discriminator Loss: 0.8856... Generator Loss: 2.3278...\n",
      "Epoch 57/100... Discriminator Loss: 0.8597... Generator Loss: 2.8705...\n",
      "Epoch 58/100... Discriminator Loss: 0.8534... Generator Loss: 2.3726...\n",
      "Epoch 59/100... Discriminator Loss: 1.0179... Generator Loss: 1.8151...\n",
      "Epoch 60/100... Discriminator Loss: 0.7133... Generator Loss: 2.2581...\n",
      "Epoch 61/100... Discriminator Loss: 0.9656... Generator Loss: 1.8519...\n",
      "Epoch 62/100... Discriminator Loss: 0.9883... Generator Loss: 1.8178...\n",
      "Epoch 63/100... Discriminator Loss: 0.9223... Generator Loss: 2.3644...\n",
      "Epoch 64/100... Discriminator Loss: 0.7674... Generator Loss: 2.4019...\n",
      "Epoch 65/100... Discriminator Loss: 0.9235... Generator Loss: 1.9355...\n",
      "Epoch 66/100... Discriminator Loss: 0.9762... Generator Loss: 1.9689...\n",
      "Epoch 67/100... Discriminator Loss: 0.9091... Generator Loss: 1.8464...\n",
      "Epoch 68/100... Discriminator Loss: 1.0446... Generator Loss: 1.4970...\n",
      "Epoch 69/100... Discriminator Loss: 0.8714... Generator Loss: 2.4258...\n",
      "Epoch 70/100... Discriminator Loss: 0.9105... Generator Loss: 2.2633...\n",
      "Epoch 71/100... Discriminator Loss: 0.9615... Generator Loss: 2.0209...\n",
      "Epoch 72/100... Discriminator Loss: 0.9286... Generator Loss: 1.9388...\n",
      "Epoch 73/100... Discriminator Loss: 0.8193... Generator Loss: 2.1048...\n",
      "Epoch 74/100... Discriminator Loss: 1.1469... Generator Loss: 2.3447...\n",
      "Epoch 75/100... Discriminator Loss: 1.0962... Generator Loss: 1.6182...\n",
      "Epoch 76/100... Discriminator Loss: 0.8851... Generator Loss: 2.0793...\n",
      "Epoch 77/100... Discriminator Loss: 1.0431... Generator Loss: 1.5453...\n",
      "Epoch 78/100... Discriminator Loss: 0.8506... Generator Loss: 2.1421...\n",
      "Epoch 79/100... Discriminator Loss: 0.9537... Generator Loss: 1.9606...\n",
      "Epoch 80/100... Discriminator Loss: 0.8767... Generator Loss: 2.0985...\n",
      "Epoch 81/100... Discriminator Loss: 0.8469... Generator Loss: 2.3338...\n",
      "Epoch 82/100... Discriminator Loss: 1.0023... Generator Loss: 2.1719...\n",
      "Epoch 83/100... Discriminator Loss: 0.8495... Generator Loss: 1.9986...\n",
      "Epoch 84/100... Discriminator Loss: 0.8704... Generator Loss: 1.8342...\n",
      "Epoch 85/100... Discriminator Loss: 0.8414... Generator Loss: 1.9977...\n",
      "Epoch 86/100... Discriminator Loss: 0.9310... Generator Loss: 1.5758...\n",
      "Epoch 87/100... Discriminator Loss: 0.9699... Generator Loss: 1.7508...\n",
      "Epoch 88/100... Discriminator Loss: 0.9049... Generator Loss: 2.0059...\n",
      "Epoch 89/100... Discriminator Loss: 1.0387... Generator Loss: 1.4824...\n",
      "Epoch 90/100... Discriminator Loss: 1.0708... Generator Loss: 1.5182...\n",
      "Epoch 91/100... Discriminator Loss: 1.0093... Generator Loss: 1.7206...\n",
      "Epoch 92/100... Discriminator Loss: 0.8619... Generator Loss: 2.1300...\n",
      "Epoch 93/100... Discriminator Loss: 0.9150... Generator Loss: 1.8191...\n",
      "Epoch 94/100... Discriminator Loss: 0.9286... Generator Loss: 1.9776...\n",
      "Epoch 95/100... Discriminator Loss: 0.8848... Generator Loss: 2.2649...\n",
      "Epoch 96/100... Discriminator Loss: 0.8829... Generator Loss: 1.7499...\n",
      "Epoch 97/100... Discriminator Loss: 1.0445... Generator Loss: 1.9663...\n",
      "Epoch 98/100... Discriminator Loss: 1.0639... Generator Loss: 1.6922...\n",
      "Epoch 99/100... Discriminator Loss: 0.9525... Generator Loss: 1.7503...\n",
      "Epoch 100/100... Discriminator Loss: 0.9366... Generator Loss: 2.0478...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-c172c3e731a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_samples.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'f'"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 100\n",
    "samples = []\n",
    "losses = []\n",
    "\n",
    "saver = tf.train.Saver(var_list=g_vars)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for ii in range(mnist.train.num_examples//batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # reshape and rescale image to certain dim\n",
    "            batch_images = batch[0].reshape((batch_size, 784))\n",
    "            batch_images = batch_images*2 - 1\n",
    "            \n",
    "            # Sample random noise for G\n",
    "            batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "            \n",
    "            # Run Optimizers\n",
    "            _ = sess.run(d_train_opt, feed_dict={input_real: batch_images, input_z: batch_z})\n",
    "            _ = sess.run(g_train_opt, feed_dict={input_z: batch_z})\n",
    "        \n",
    "        # At the each end of the epoch, get the losses and print out\n",
    "        train_loss_d = sess.run(d_loss, {input_z: batch_z, input_real: batch_images})\n",
    "        train_loss_g = g_loss.eval({input_z: batch_z})\n",
    "        \n",
    "        print(\"Epoch {}/{}...\".format(e + 1, epochs),\n",
    "              \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "              \"Generator Loss: {:.4f}...\".format(train_loss_g))\n",
    "        \n",
    "        # Save losses to view after training\n",
    "        losses.append((train_loss_d, train_loss_g))\n",
    "        \n",
    "        # Sample from generator\n",
    "        sample_z = np.random.uniform(-1, 1, size=(16, z_size))\n",
    "        gen_samples = sess.run(\n",
    "                      generator(input_z, input_size, reuse=True),\n",
    "                      feed_dict={input_z: sample_z})\n",
    "        samples.append(gen_samples)\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save training generator samples\n",
    "with open('train_samples.pkl', 'wb') as f:\n",
    "    pkl.dump(samples,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npCYMviQ18f+GmWq6NWLIwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## View training loss\n",
    "fig, ax = plt.subplots()\n",
    "print(len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
